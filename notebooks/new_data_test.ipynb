{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load libraries"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Import the required libraries."
  },
  {
   "metadata": {
    "id": "1Mf4kbp8yOxd",
    "execution": {
     "iopub.status.busy": "2024-04-19T20:47:27.814529Z",
     "iopub.execute_input": "2024-04-19T20:47:27.815337Z",
     "iopub.status.idle": "2024-04-19T20:47:27.821956Z",
     "shell.execute_reply.started": "2024-04-19T20:47:27.815293Z",
     "shell.execute_reply": "2024-04-19T20:47:27.821163Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-09T23:01:48.310450Z",
     "start_time": "2024-10-09T23:01:43.960040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from typing import Dict, List, Optional, Text, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.keras.utils.losses_utils import reduce_weighted_loss\n",
    "\n",
    "import kagglehub"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 17:01:45.611512: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-09 17:01:45.688596: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-09 17:01:45.710244: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-09 17:01:45.858842: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-09 17:01:47.113633: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/dtlitster/Documents/Random/Random/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T22:56:38.090257Z",
     "start_time": "2024-10-09T22:56:37.711552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"fantineh/next-day-wildfire-spread\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/dtlitster/.cache/kagglehub/datasets/fantineh/next-day-wildfire-spread/versions/2\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prepare data"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T23:03:07.575582Z",
     "start_time": "2024-10-09T23:03:07.566597Z"
    }
   },
   "cell_type": "code",
   "source": "from data import new_data_utils as ndu",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "id": "X1jBBEinQbM0",
    "execution": {
     "iopub.status.busy": "2024-04-19T20:47:29.144326Z",
     "iopub.execute_input": "2024-04-19T20:47:29.144587Z",
     "iopub.status.idle": "2024-04-19T20:47:30.039004Z",
     "shell.execute_reply.started": "2024-04-19T20:47:29.144556Z",
     "shell.execute_reply": "2024-04-19T20:47:30.038393Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-09T23:03:35.643564Z",
     "start_time": "2024-10-09T23:03:34.942950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BATCH_SIZE = 128\n",
    "path = \"/home/dtlitster/.cache/kagglehub/datasets/fantineh/next-day-wildfire-spread/versions/2\"\n",
    "train_dataset = ndu.get_dataset(path + \"*\", \n",
    "    data_size=64, sample_size=32, batch_size=BATCH_SIZE,\n",
    "    num_in_channels=12, compression_type=None, clip_and_normalize=True,\n",
    "    clip_and_rescale=False, random_crop=True, center_crop=False)\n",
    "# \n",
    "# validation_dataset = get_dataset('/kaggle/input/next-day-wildfire-spread/next_day_wildfire_spread_eval*', \n",
    "#     data_size=64, sample_size=32, batch_size=BATCH_SIZE,\n",
    "#     num_in_channels=12, compression_type=None, clip_and_normalize=True,\n",
    "#     clip_and_rescale=False, random_crop=True, center_crop=False)\n",
    "# \n",
    "# test_dataset = get_dataset('/kaggle/input/next-day-wildfire-spread/next_day_wildfire_spread_test*',\n",
    "#     data_size=64, sample_size=64, batch_size=BATCH_SIZE,\n",
    "#     num_in_channels=12, compression_type=None, clip_and_normalize=True,\n",
    "#     clip_and_rescale=False, random_crop=False, center_crop=False)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T23:03:38.462798Z",
     "start_time": "2024-10-09T23:03:38.459535Z"
    }
   },
   "cell_type": "code",
   "source": "print(train_dataset)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 32, 32, 12), dtype=tf.float32, name=None), TensorSpec(shape=(None, 32, 32, 1), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Visualize data\n",
    "We will check content of the dataset by plotting them"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's plot the data!\n",
    "\n",
    "First we define the names for each of our variables."
   ]
  },
  {
   "metadata": {
    "id": "bnG0_l_ChjUt",
    "execution": {
     "iopub.status.busy": "2024-04-19T20:47:30.041193Z",
     "iopub.execute_input": "2024-04-19T20:47:30.041579Z",
     "iopub.status.idle": "2024-04-19T20:47:30.046221Z",
     "shell.execute_reply.started": "2024-04-19T20:47:30.041533Z",
     "shell.execute_reply": "2024-04-19T20:47:30.045426Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-09T23:03:56.419255Z",
     "start_time": "2024-10-09T23:03:56.413544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TITLES = [\n",
    "  'Elevation',\n",
    "  'Wind\\ndirection',\n",
    "  'Wind\\nvelocity',\n",
    "  'Min\\ntemp',\n",
    "  'Max\\ntemp',\n",
    "  'Humidity',\n",
    "  'Precip',\n",
    "  'Drought',\n",
    "  'Vegetation',\n",
    "  'Population\\ndensity',\n",
    "  'Energy\\nrelease\\ncomponent',\n",
    "  'Previous\\nfire\\nmask',\n",
    "  'Fire\\nmask'\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define some helper variables for the plot. "
  },
  {
   "metadata": {
    "id": "sPtKQzQv71J_",
    "outputId": "6694ad6e-0044-4fbc-b184-f615ac14a885",
    "execution": {
     "iopub.status.busy": "2024-04-19T20:47:30.047297Z",
     "iopub.execute_input": "2024-04-19T20:47:30.047897Z",
     "iopub.status.idle": "2024-04-19T20:47:30.058578Z",
     "shell.execute_reply.started": "2024-04-19T20:47:30.047857Z",
     "shell.execute_reply": "2024-04-19T20:47:30.05789Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-09T23:03:57.345688Z",
     "start_time": "2024-10-09T23:03:57.340088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_samples_from_dataset(dataset: tf.data.Dataset, n_rows: int):\n",
    "    \"\"\"\n",
    "    Plot 'n_rows' rows of samples from dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset (Dataset): Dataset from which to plot samples.\n",
    "        n_rows (int): Number of rows to plot.\n",
    "    \"\"\"\n",
    "    global TITLES\n",
    "    \n",
    "    # Get batch\n",
    "    inputs, labels = None, None\n",
    "    for elem in dataset:\n",
    "        inputs, labels = elem\n",
    "        break\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,6.5))\n",
    "\n",
    "    # Variables for controllong the color map for the fire masks\n",
    "    CMAP = colors.ListedColormap(['black', 'silver', 'orangered'])\n",
    "    BOUNDS = [-1, -0.1, 0.001, 1]\n",
    "    NORM = colors.BoundaryNorm(BOUNDS, CMAP.N)\n",
    "    # Number of data variables\n",
    "    n_features = 12\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_features + 1):\n",
    "            plt.subplot(n_rows, n_features + 1, i * (n_features + 1) + j + 1)\n",
    "            if i == 0:\n",
    "                plt.title(TITLES[j], fontsize=13)\n",
    "            if j < n_features - 1:\n",
    "                plt.imshow(inputs[i, :, :, j], cmap='viridis')\n",
    "            if j == n_features - 1:\n",
    "                plt.imshow(inputs[i, :, :, -1], cmap=CMAP, norm=NORM)\n",
    "            if j == n_features:\n",
    "                plt.imshow(labels[i, :, :, 0], cmap=CMAP, norm=NORM) \n",
    "            plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T20:47:30.1755Z",
     "iopub.execute_input": "2024-04-19T20:47:30.175741Z",
     "iopub.status.idle": "2024-04-19T20:47:33.790105Z",
     "shell.execute_reply.started": "2024-04-19T20:47:30.175714Z",
     "shell.execute_reply": "2024-04-19T20:47:33.789337Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-09T23:03:59.591915Z",
     "start_time": "2024-10-09T23:03:58.925157Z"
    }
   },
   "cell_type": "code",
   "source": "plot_samples_from_dataset(train_dataset, 5)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 17:03:59.045333: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: /home/dtlitster/.cache/kagglehub/datasets/fantineh/next-day-wildfire-spread/versions/2; Is a directory\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} /home/dtlitster/.cache/kagglehub/datasets/fantineh/next-day-wildfire-spread/versions/2; Is a directory [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFailedPreconditionError\u001B[0m                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mplot_samples_from_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[8], line 13\u001B[0m, in \u001B[0;36mplot_samples_from_dataset\u001B[0;34m(dataset, n_rows)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Get batch\u001B[39;00m\n\u001B[1;32m     12\u001B[0m inputs, labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43melem\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43melem\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mbreak\u001B[39;49;00m\n",
      "File \u001B[0;32m~/Documents/Random/Random/.venv/lib/python3.12/site-packages/tensorflow/python/data/ops/iterator_ops.py:826\u001B[0m, in \u001B[0;36mOwnedIterator.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    824\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__next__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    825\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 826\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_internal\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    827\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m errors\u001B[38;5;241m.\u001B[39mOutOfRangeError:\n\u001B[1;32m    828\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Random/Random/.venv/lib/python3.12/site-packages/tensorflow/python/data/ops/iterator_ops.py:776\u001B[0m, in \u001B[0;36mOwnedIterator._next_internal\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    773\u001B[0m \u001B[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001B[39;00m\n\u001B[1;32m    774\u001B[0m \u001B[38;5;66;03m# to communicate that there is no more data to iterate over.\u001B[39;00m\n\u001B[1;32m    775\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context\u001B[38;5;241m.\u001B[39mexecution_mode(context\u001B[38;5;241m.\u001B[39mSYNC):\n\u001B[0;32m--> 776\u001B[0m   ret \u001B[38;5;241m=\u001B[39m \u001B[43mgen_dataset_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterator_get_next\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    777\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_iterator_resource\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    778\u001B[0m \u001B[43m      \u001B[49m\u001B[43moutput_types\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_output_types\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    779\u001B[0m \u001B[43m      \u001B[49m\u001B[43moutput_shapes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_output_shapes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    781\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    782\u001B[0m     \u001B[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001B[39;00m\n\u001B[1;32m    783\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_element_spec\u001B[38;5;241m.\u001B[39m_from_compatible_tensor_list(ret)  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Random/Random/.venv/lib/python3.12/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3086\u001B[0m, in \u001B[0;36miterator_get_next\u001B[0;34m(iterator, output_types, output_shapes, name)\u001B[0m\n\u001B[1;32m   3084\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _result\n\u001B[1;32m   3085\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m-> 3086\u001B[0m   \u001B[43m_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_from_not_ok_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3087\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_FallbackException:\n\u001B[1;32m   3088\u001B[0m   \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Random/Random/.venv/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:5983\u001B[0m, in \u001B[0;36mraise_from_not_ok_status\u001B[0;34m(e, name)\u001B[0m\n\u001B[1;32m   5981\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mraise_from_not_ok_status\u001B[39m(e, name) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m NoReturn:\n\u001B[1;32m   5982\u001B[0m   e\u001B[38;5;241m.\u001B[39mmessage \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m name: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(name \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m-> 5983\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_status_to_exception(e) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mFailedPreconditionError\u001B[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} /home/dtlitster/.cache/kagglehub/datasets/fantineh/next-day-wildfire-spread/versions/2; Is a directory [Op:IteratorGetNext] name: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Building testing"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Metrics\n",
    "Let's define metric calculation functions"
   ]
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T20:47:33.791866Z",
     "iopub.execute_input": "2024-04-19T20:47:33.79225Z",
     "iopub.status.idle": "2024-04-19T20:47:33.805239Z",
     "shell.execute_reply.started": "2024-04-19T20:47:33.792214Z",
     "shell.execute_reply": "2024-04-19T20:47:33.804458Z"
    },
    "trusted": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def IoU_metric(real_mask: tf.Tensor, predicted_mask: tf.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Calculation of intersection over union metric.\n",
    "    \n",
    "    Args:\n",
    "        real_mask (Tensor): Ground-truth mask\n",
    "        predicted_mask (Tensor): Mask predicted by model\n",
    "    Returns:\n",
    "        (float): IoU metric value\n",
    "    \"\"\"\n",
    "    real_mask = tf.where(real_mask < 0, 0, real_mask)\n",
    "    \n",
    "    intersection = np.logical_and(real_mask, predicted_mask)\n",
    "    union = np.logical_or(real_mask, predicted_mask)\n",
    "    \n",
    "    if np.sum(union) == 0:\n",
    "        return 1\n",
    "    return np.sum(intersection) / np.sum(union)\n",
    "\n",
    "def recall_metric(real_mask: tf.Tensor, predicted_mask: tf.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Calculation of recall metric.\n",
    "    \n",
    "    Args:\n",
    "        real_mask (Tensor): Ground-truth mask\n",
    "        predicted_mask (Tensor): Mask predicted by model\n",
    "    Returns:\n",
    "        (float): recall metric value\n",
    "    \"\"\"\n",
    "    real_mask = tf.where(real_mask < 0, 0, real_mask)\n",
    "    \n",
    "    true_positives = np.sum(np.logical_and(real_mask, predicted_mask))\n",
    "    actual_positives = np.sum(real_mask)\n",
    "    if actual_positives == 0:\n",
    "        return 1\n",
    "    \n",
    "    return true_positives / actual_positives\n",
    "\n",
    "def precision_metric(real_mask: tf.Tensor, predicted_mask: tf.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Calculation of precision metric.\n",
    "    \n",
    "    Args:\n",
    "        real_mask (Tensor): Ground-truth mask\n",
    "        predicted_mask (Tensor): Mask predicted by model\n",
    "    Returns:\n",
    "        (float): precision metric value\n",
    "    \"\"\"\n",
    "    real_mask = tf.where(real_mask < 0, 0, real_mask)\n",
    "    \n",
    "    true_positives = np.sum(np.logical_and(real_mask, predicted_mask))\n",
    "    predicted_positives = np.sum(predicted_mask)\n",
    "    if predicted_positives == 0:\n",
    "        return 1\n",
    "    \n",
    "    return true_positives / predicted_positives"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loss functions"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T20:47:33.806564Z",
     "iopub.execute_input": "2024-04-19T20:47:33.80715Z",
     "iopub.status.idle": "2024-04-19T20:47:33.821003Z",
     "shell.execute_reply.started": "2024-04-19T20:47:33.807116Z",
     "shell.execute_reply": "2024-04-19T20:47:33.82025Z"
    },
    "trusted": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def dice_coef(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Dice loss function calculator.\n",
    "    \n",
    "    Args:\n",
    "        y_true (Tensor): \n",
    "        y_pred (Tensor):\n",
    "    Returns:\n",
    "        (Tensor): Dice loss for each element of a batch.\n",
    "    \"\"\"\n",
    "    smooth = 1e-6\n",
    "    y_true_f = K.reshape(y_true, (BATCH_SIZE, -1))\n",
    "    y_pred_f = K.reshape(y_pred, (BATCH_SIZE, -1))\n",
    "    intersection = K.sum(y_true_f * y_pred_f, axis=1)\n",
    "    return 1 - (2. * intersection + smooth) / (K.sum(y_true_f, axis=1) + K.sum(y_pred_f, axis=1) + smooth)\n",
    "\n",
    "def weighted_bincrossentropy(true: tf.Tensor, pred: tf.Tensor, weight_zero: float = 0.01, weight_one: float = 1) -> float:\n",
    "    \"\"\"\n",
    "    Calculates weighted binary cross entropy. The weights are fixed.\n",
    "        \n",
    "    This can be useful for unbalanced catagories.\n",
    "    \n",
    "    Adjust the weights here depending on what is required.\n",
    "    \n",
    "    For example if there are 10x as many positive classes as negative classes,\n",
    "        if you adjust weight_zero = 1.0, weight_one = 0.1, then false positives \n",
    "        will be penalize 10 times as much as false negatives.\n",
    "    \n",
    "    Args:\n",
    "        true (Tensor): Ground-truth values\n",
    "        pred (Tensor): Predited values\n",
    "        weight_zero (float): Weight of class 0 (no-fire)\n",
    "        weight_one (float): Weight of class 1 (fire)\n",
    "        \n",
    "    \"\"\"\n",
    "  \n",
    "    # calculate the binary cross entropy\n",
    "    bin_crossentropy = K.binary_crossentropy(true, pred)\n",
    "    \n",
    "    # apply the weights\n",
    "    weights = true * weight_one + (1. - true) * weight_zero\n",
    "    weighted_bin_crossentropy = weights * bin_crossentropy \n",
    "    \n",
    "    return K.mean(weighted_bin_crossentropy, axis=1)\n",
    "\n",
    "def bce_dice_loss(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
    "    \"\"\"\n",
    "    BCE loss function calculator.\n",
    "    \n",
    "    Args:\n",
    "        y_true (Tensor): \n",
    "        y_pred (Tensor):\n",
    "    Returns:\n",
    "        (Tensor): Mean BCE Dice loss over a batch.\n",
    "    \"\"\"\n",
    "    \n",
    "    y_true_f = K.reshape(y_true, (BATCH_SIZE, -1))\n",
    "    y_pred_f = K.reshape(y_pred, (BATCH_SIZE, -1))\n",
    "    return reduce_weighted_loss(weighted_bincrossentropy(y_true_f, y_pred_f) + dice_coef(y_true, y_pred))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluation loop\n",
    "Let's define evaluation process over specified dataset"
   ]
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T20:47:33.822559Z",
     "iopub.execute_input": "2024-04-19T20:47:33.822774Z",
     "iopub.status.idle": "2024-04-19T20:47:33.836667Z",
     "shell.execute_reply.started": "2024-04-19T20:47:33.822748Z",
     "shell.execute_reply": "2024-04-19T20:47:33.836024Z"
    },
    "trusted": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_model(prediction_function: Callable[[tf.Tensor], tf.Tensor],\n",
    "                   eval_dataset: tf.data.Dataset) -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Loads dataset according to file pattern and evaluates model's predictions on it.\n",
    "    \n",
    "    Parameters:\n",
    "        model (Callable[[tf.Tensor], tf.Tensor]): Function for model inference.\n",
    "        eval_dataset (tf.dataDataset): Dataset for evaluation.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[float, float, float, float]: IoU score, recall score, precision score and mean loss.\n",
    "    \"\"\"\n",
    "    IoU_measures = []\n",
    "    recall_measures = []\n",
    "    precision_measures = []\n",
    "    losses = []\n",
    "    \n",
    "    for inputs, labels in tqdm(eval_dataset):\n",
    "        # Prediction shape (N, W, H)\n",
    "        predictions = prediction_function(inputs)\n",
    "        for i in range(inputs.shape[0]):\n",
    "            IoU_measures.append(IoU_metric(labels[i, :, :,  0], predictions[i, :, :]))\n",
    "            recall_measures.append(recall_metric(labels[i, :, :,  0], predictions[i, :, :]))\n",
    "            precision_measures.append(precision_metric(labels[i, :, :,  0], predictions[i, :, :]))\n",
    "        labels_cleared = tf.where(labels < 0, 0, labels)\n",
    "        losses.append(bce_dice_loss(labels_cleared, tf.expand_dims(tf.cast(predictions, tf.float32), axis=-1)))\n",
    "            \n",
    "    mean_IoU = np.mean(IoU_measures)\n",
    "    mean_recall = np.mean(recall_measures)\n",
    "    mean_precision = np.mean(precision_measures)\n",
    "    mean_loss = np.mean(losses)\n",
    "    return mean_IoU, mean_recall, mean_precision, mean_loss"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Modeling technique\n",
    "Description of technique"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T20:47:33.837619Z",
     "iopub.execute_input": "2024-04-19T20:47:33.837824Z",
     "iopub.status.idle": "2024-04-19T20:47:46.398153Z",
     "shell.execute_reply.started": "2024-04-19T20:47:33.837799Z",
     "shell.execute_reply": "2024-04-19T20:47:46.397382Z"
    },
    "trusted": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install git+https://github.com/tensorflow/examples.git"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T20:47:46.399777Z",
     "iopub.execute_input": "2024-04-19T20:47:46.400106Z",
     "iopub.status.idle": "2024-04-19T20:47:48.1111Z",
     "shell.execute_reply.started": "2024-04-19T20:47:46.400037Z",
     "shell.execute_reply": "2024-04-19T20:47:48.110358Z"
    },
    "trusted": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "def build_CNN_AE_model() -> Model:\n",
    "    \"\"\"\n",
    "    Create CNN auto encode model.\n",
    "    \n",
    "    Returns:\n",
    "        (Model): Keras model.\n",
    "    \"\"\"\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=[32, 32, 12], include_top=False, weights=None)\n",
    "\n",
    "    # Use the activations of these layers\n",
    "    layer_names = [\n",
    "        'block_1_expand_relu',   # 32x32\n",
    "        'block_3_expand_relu',   # 16x16\n",
    "        'block_6_expand_relu',   # 8x8\n",
    "        'block_13_expand_relu',  # 4x4\n",
    "        'block_16_project',      # 2x2\n",
    "    ]\n",
    "    base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "    # Create the feature extraction model\n",
    "    down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
    "\n",
    "    down_stack.trainable = True\n",
    "\n",
    "    up_stack = [\n",
    "        pix2pix.upsample(512, 3),  # 2x2 -> 4x4\n",
    "        pix2pix.upsample(256, 3),  # 4x4 -> 8x8\n",
    "        pix2pix.upsample(128, 3),  # 8x8 -> 16x16\n",
    "        pix2pix.upsample(64, 3),   # 16x16 -> 32x32\n",
    "    ]\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=[32, 32, 12])\n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = down_stack(inputs)\n",
    "    x = skips[-1]\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        concat = tf.keras.layers.Concatenate()\n",
    "        x = concat([x, skip])\n",
    "\n",
    "    # This is the last layer of the model\n",
    "    last = tf.keras.layers.Conv2DTranspose(\n",
    "        filters=1, kernel_size=3, strides=2,\n",
    "    padding='same')  #64x64 -> 128x128\n",
    "\n",
    "    x = last(x)\n",
    "    outputs = Conv2D(1, 1, padding='same', activation='sigmoid')(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "# Create the segmentation model\n",
    "segmentation_model = build_CNN_AE_model()\n",
    "segmentation_model.summary()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training loop"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T20:47:48.113374Z",
     "iopub.execute_input": "2024-04-19T20:47:48.113952Z",
     "iopub.status.idle": "2024-04-19T20:54:16.101074Z",
     "shell.execute_reply.started": "2024-04-19T20:47:48.113907Z",
     "shell.execute_reply": "2024-04-19T20:54:16.100348Z"
    },
    "trusted": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_model(model: Model, train_dataset: tf.data.Dataset, epochs:int=10) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Trains a model using train dataset. (Save weights of model with best IoU)\n",
    "    \n",
    "    Args:\n",
    "        model (Model): Model to train.\n",
    "        train_dataset (Dataset): Training dataset.\n",
    "        epochs (int): Number of epochs\n",
    "    Returns:\n",
    "        Tuple[List[float], List[float]]: Train losses and Validation losses\n",
    "    \"\"\"\n",
    "    loss_fn = bce_dice_loss\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    batch_losses = []\n",
    "    val_losses = []\n",
    "    best_IoU = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        losses = []\n",
    "        print(f'Epoch {epoch+1}/{epochs}')\n",
    "        # Iterate through the dataset\n",
    "        progress = tqdm(train_dataset)\n",
    "        for images, masks in progress:\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Forward pass\n",
    "                predictions = model(images, training=True)\n",
    "                label = tf.where(masks < 0, 0, masks)\n",
    "                # Compute the loss\n",
    "                loss = loss_fn(label, predictions)\n",
    "                losses.append(loss.numpy())\n",
    "                progress.set_postfix({'batch_loss': loss.numpy()})\n",
    "            # Compute gradients\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            # Update the model's weights\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        # Evaluate model\n",
    "        print(\"Evaluation...\")\n",
    "        IoU, recall, precision, val_loss = evaluate_model(lambda x: tf.where(model.predict(x) > 0.5, 1, 0)[:,:,:,0], validation_dataset)\n",
    "        print(\"Validation set metrics:\")\n",
    "        print(f\"Mean IoU: {IoU}\\nMean precision: {precision}\\nMean recall: {recall}\\nValidation loss: {val_loss}\\n\")\n",
    "        # Save best model\n",
    "        if IoU > best_IoU:\n",
    "            best_IoU = IoU\n",
    "            model.save_weights(\"best.h5\")\n",
    "        \n",
    "        # Print the loss for monitoring\n",
    "        print(f'Epoch: {epoch}, Train loss: {np.mean(losses)}')\n",
    "        batch_losses.append(np.mean(losses))\n",
    "        val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Best model IoU: {best_IoU}\")\n",
    "    return batch_losses, val_losses\n",
    "\n",
    "# Set reproducability\n",
    "tf.random.set_seed(1337)\n",
    "\n",
    "segmentation_model = build_CNN_AE_model()\n",
    "train_losses, val_losses = train_model(segmentation_model, train_dataset, epochs=15)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Plot loss functions"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T21:01:51.97239Z",
     "iopub.execute_input": "2024-04-19T21:01:51.973168Z",
     "iopub.status.idle": "2024-04-19T21:01:52.326079Z",
     "shell.execute_reply.started": "2024-04-19T21:01:51.973126Z",
     "shell.execute_reply": "2024-04-19T21:01:52.325333Z"
    },
    "trusted": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_train_and_val_losses(train_losses, val_losses):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    axs[0].plot(train_losses)\n",
    "    axs[0].set_title(\"train loss\")\n",
    "    \n",
    "    axs[1].plot(val_losses)\n",
    "    axs[1].set_title(\"validation loss\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_train_and_val_losses(train_losses, val_losses)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Testing"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T21:01:58.621592Z",
     "iopub.execute_input": "2024-04-19T21:01:58.621852Z",
     "iopub.status.idle": "2024-04-19T21:02:00.165487Z",
     "shell.execute_reply.started": "2024-04-19T21:01:58.621823Z",
     "shell.execute_reply": "2024-04-19T21:02:00.164834Z"
    },
    "trusted": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load best model\n",
    "segmentation_model = build_CNN_AE_model()\n",
    "segmentation_model.load_weights(\"best.h5\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Metrics on test set\n",
    "Calculating metrics on test set"
   ]
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T21:02:00.166935Z",
     "iopub.execute_input": "2024-04-19T21:02:00.167188Z",
     "iopub.status.idle": "2024-04-19T21:02:08.257751Z",
     "shell.execute_reply.started": "2024-04-19T21:02:00.167157Z",
     "shell.execute_reply": "2024-04-19T21:02:08.256975Z"
    },
    "trusted": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def predict_on_full_image(x_batch: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Do model inference of full 64x64 image.\n",
    "    \n",
    "    Args:\n",
    "        x_batch (tf.Tensor): Input batch with 64x64 images.\n",
    "    Returns:\n",
    "        (tf.Tensor): Prediction 64x64 image mask.\n",
    "    \"\"\"\n",
    "    global segmentation_model\n",
    "    b_size = x_batch.shape[0]\n",
    "    top_left = x_batch[:, :32, :32]\n",
    "    top_right = x_batch[:, :32, 32:]\n",
    "    bottom_left = x_batch[:, 32:, :32]\n",
    "    bottom_right = x_batch[:, 32:, 32:]\n",
    "\n",
    "    stacked_tensor = tf.stack([top_left, top_right, bottom_left, bottom_right], axis=1)\n",
    "    stacked_tensor = tf.reshape(stacked_tensor, (4*b_size, 32, 32, -1))\n",
    "    result_tensor = segmentation_model.predict(stacked_tensor)\n",
    "\n",
    "    reshaped_tensor = tf.reshape(result_tensor, (b_size, 2, 2, 32, 32, -1))\n",
    "    reconstructed_tensor = tf.concat([\n",
    "        tf.concat([reshaped_tensor[:, 0, 0], reshaped_tensor[:, 0, 1]], axis=2),\n",
    "        tf.concat([reshaped_tensor[:, 1, 0], reshaped_tensor[:, 1, 1]], axis=2)\n",
    "    ], axis=1)\n",
    "    return tf.where(reconstructed_tensor[:, :, :, 0] > 0.5, 1, 0)\n",
    "\n",
    "print(\"Evaluation...\")\n",
    "print(\"Test set metrics:\")\n",
    "IoU, recall, precision, val_loss = evaluate_model(predict_on_full_image, test_dataset)\n",
    "print(f\"Mean IoU: {IoU}\\nMean precision: {precision}\\nMean recall: {recall}\\nTest loss: {val_loss}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Comparison with statistical model\n",
    "How better is our model in comparison of prediction of only high frequency fire cells (>0.2 probability of being on fire)"
   ]
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T21:02:16.166759Z",
     "iopub.execute_input": "2024-04-19T21:02:16.167569Z",
     "iopub.status.idle": "2024-04-19T21:02:19.831093Z",
     "shell.execute_reply.started": "2024-04-19T21:02:16.16752Z",
     "shell.execute_reply": "2024-04-19T21:02:19.830239Z"
    },
    "trusted": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class NaivePredictor:\n",
    "    \"\"\"\n",
    "    Naive predictor that predicts fire only if cell has chance being on fire more than 0.2\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize model and create frequency matrix\n",
    "        \"\"\"\n",
    "        self.frequency_matrix = tf.zeros((32, 32), dtype=np.float32)\n",
    "    \n",
    "    def train(self, train_dataset: tf.data.Dataset) -> None:\n",
    "        \"\"\"\n",
    "        Train by calculating frequency for each cell.\n",
    "        \n",
    "        Args:\n",
    "            train_dataset (Dataset): Dataset to train on.\n",
    "        \"\"\"\n",
    "        for _, labels in tqdm(train_dataset):\n",
    "            label_batch = labels[:, :, :, 0]\n",
    "            label_batch = tf.where(label_batch < 0, 0, label_batch)\n",
    "            self.frequency_matrix = self.frequency_matrix + np.sum(label_batch, axis=0)\n",
    "        self.frequency_matrix = self.frequency_matrix / np.max(self.frequency_matrix)\n",
    "        self.frequency_matrix = tf.where(self.frequency_matrix > 0.2, 1, 0)\n",
    "    \n",
    "    def predict(self, X: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Dummy predict function.\n",
    "        \n",
    "        Args:\n",
    "            train_dataset (Dataset): Dataset to train on.\n",
    "        Returns:\n",
    "            (Tensor): Predicted fire mask.\n",
    "        \"\"\"\n",
    "        return tf.tile(tf.expand_dims(self.frequency_matrix, axis=0), [X.shape[0],1,1])\n",
    "\n",
    "naive_predictor = NaivePredictor()\n",
    "naive_predictor.train(train_dataset)"
   ]
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-12T22:31:06.397409Z",
     "iopub.execute_input": "2024-04-12T22:31:06.397667Z",
     "iopub.status.idle": "2024-04-12T22:31:10.05263Z",
     "shell.execute_reply.started": "2024-04-12T22:31:06.397632Z",
     "shell.execute_reply": "2024-04-12T22:31:10.051841Z"
    },
    "trusted": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "IoU, recall, precision, val_loss = evaluate_model(naive_predictor.predict, test_dataset)\n",
    "print(f\"Mean IoU: {IoU}\\nMean precision: {precision}\\nMean recall: {recall}\\nTest loss: {val_loss}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Compare trained model with Naive one: <br/>\n",
    "1. <font color=\"green\">IoU 23% better</font>\n",
    "2. <font color=\"green\">Precision 47% better</font>\n",
    "3. <font color=\"red\">Recall 50% worse</font>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference on test set"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T21:02:24.114209Z",
     "iopub.execute_input": "2024-04-19T21:02:24.114509Z",
     "iopub.status.idle": "2024-04-19T21:02:24.125395Z",
     "shell.execute_reply.started": "2024-04-19T21:02:24.114475Z",
     "shell.execute_reply": "2024-04-19T21:02:24.124475Z"
    },
    "trusted": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def show_inference(n_rows: int, features: tf.Tensor, label: tf.Tensor, prediction_function: Callable[[tf.Tensor], tf.Tensor]) -> None:\n",
    "    \"\"\"\n",
    "    Show model inference through images.\n",
    "    \n",
    "    Args:\n",
    "        n_rows (int): Number of rows for subplots.\n",
    "        features (tf.Tensor): Input features.\n",
    "        label (tf.Tensor): True labels.\n",
    "        prediction_function (Callable[[tf.Tensor], tf.Tensor]): Function for model prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Variables for controllong the color map for the fire masks\n",
    "    CMAP = colors.ListedColormap(['black', 'silver', 'orangered'])\n",
    "    BOUNDS = [-1, -0.1, 0.001, 1]\n",
    "    NORM = colors.BoundaryNorm(BOUNDS, CMAP.N)\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,n_rows*4))\n",
    "    \n",
    "    prediction = prediction_function(features)\n",
    "    for i in range(n_rows):\n",
    "        plt.subplot(n_rows, 3, i*3 + 1)\n",
    "        plt.title(\"Previous day fire\")\n",
    "        plt.imshow(features[i, :, :, -1], cmap=CMAP, norm=NORM)\n",
    "        plt.axis('off')\n",
    "        plt.subplot(n_rows, 3, i*3 + 2)\n",
    "        plt.title(\"True next day fire\")\n",
    "        plt.imshow(label[i, :, :, 0], cmap=CMAP, norm=NORM)\n",
    "        plt.axis('off')\n",
    "        plt.subplot(n_rows, 3, i*3 + 3)\n",
    "        plt.title(\"Predicted next day fire\")\n",
    "        plt.imshow(prediction[i, :, :])\n",
    "        plt.axis('off')    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-19T21:03:48.107354Z",
     "iopub.execute_input": "2024-04-19T21:03:48.107657Z",
     "iopub.status.idle": "2024-04-19T21:03:49.558023Z",
     "shell.execute_reply.started": "2024-04-19T21:03:48.107624Z",
     "shell.execute_reply": "2024-04-19T21:03:49.557206Z"
    },
    "trusted": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "features, labels = next(iter(test_dataset))\n",
    "show_inference(5, features, labels, predict_on_full_image)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": "",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fire_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
